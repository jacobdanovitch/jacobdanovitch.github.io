<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>End-To-End Memory Networks</title>
    <meta charset="utf-8" />
    <meta name="author" content="Presented by Jacob Danovitch and Kellin Pelrine" />
    <script src="libs/header-attrs-2.6/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="templates/css/mila.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, title-slide, title-slide

# End-To-End Memory Networks
## Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus
### Presented by Jacob Danovitch and Kellin Pelrine
### McGill University

---




class: inverse, center, middle

# Introduction

&lt;hr&gt;

---

# Problem Statement

Developing models capable of "reasoning" has long been a goal of AI and deep learning researchers. While this is a loose term, it can be broken into several key types.

--

**Sequential reasoning**: Processing sequential data with long-range dependencies

--

**Compositional reasoning**: Combining multiple concepts present in the data to form novel concepts

--

**Symbolic reasoning**: Manipulating explicit, human-readable symbols present in the data

--

These are all important components of human(-like) intelligence. Deep learning methods face several challenges on this front, particularly in the scopes of compositional and symbolic reasoning.

???

* Symbolic reasoning
  * Applicability to algorithm execution, theorem proving, constrained text generation
  * Things that need *exact* answers

---

# bAbI Dataset

The bAbI dataset (different paper, same authors) was designed to measure certain human-like reasoning capabilities of dialog agents:


&gt; [...] a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks **measure understanding** in several ways: whether a system is able to **answer questions via chaining facts, simple induction, deduction** and many more. The tasks are designed to be prerequisites for any system that aims to be **capable of conversing with a human**.


.footnote[
[Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks](https://arxiv.org/abs/1502.05698)
]

---

# bAbI Dataset

.middle-content[
|                                                          |                                                           |                                                                        |
|:---------------------------------------------------------|:----------------------------------------------------------|:-----------------------------------------------------------------------|
| Sam walks into the kitchen                               | Brian is a lion                                           | Mary journeyed to the den.                                             |
| Sam picks up an apple                                    | Julius is a lion                                          | Mary went back to the kitchen.                                         |
| Sam walks into the bedroom                               | Julius is white                                           | John journeyed to the bedroom.                                         |
| Sam drops the apple                                      | Bernhard is green                                         | Mary discarded the milk.                                               |
| &lt;span style="color: blue"&gt;Q: Where is the apple? &lt;/span&gt; | &lt;span style="color: blue"&gt;Q: What color is Brian? &lt;/span&gt; | &lt;span style="color: blue"&gt;Q: Where was the milk before the den?&lt;/span&gt; |
| &lt;span style="color: red"&gt;A: Bedroom &lt;/span&gt;              | &lt;span style="color: red"&gt;A: White &lt;/span&gt;                 | &lt;span style="color: red"&gt;A: Hallway&lt;/span&gt;                             |
]

???

* Tie back to sequential/compositional/symbolic logic

---

# Task Definition

## Data format

* Story `\(x = \{x_{1}, x_{2}, ..., x_{n}\}\)`
  * Each `\(x_i\)` is a _sentence_ of _multiple words_; `\(x_i = \{x_{i1}, x_{i2}, ..., x_{in}\}\)`
* Query &lt;span style="color: blue"&gt;
`\(q\)`
&lt;/span&gt;
* Answer &lt;span style="color: red"&gt;
`\(a\)`
&lt;/span&gt;

## Goal

* Correctly predict the answer `\(a\)` given `\(q, x\)`
  * Or, maximize `\(p(a|q,x)\)`
* Each `\(a\)` is one word from the bAbI vocabulary
  * More of a classification task than generation/language modeling
  
???

**double check the classification part**

---

# Contributions

.middle-content[
The authors present the End-to-End Memory Network as a method for sequentially reading from a large, external memory. The model uses soft attention to retrieve and compose symbols from memory, updating an internal state over multiple timesteps (or "hops").

The authors demonstrate that the model can be trained end-to-end with backpropagation - in contrast to hard attention-based alternatives - and achieves strong performance on bAbI. They also demonstrate the importance of multi-hop reasoning and propose several extensions for improving efficiency and performance. 
]

---

class: inverse, center, middle

# Proposed Method

&lt;hr&gt;

End-to-End Memory Networks

---

# Model Architecture

.middle-content.center[
![](https://paperswithcode.com/media/methods/new_memRNN4.jpg)
]

???

a. A single layer version of our model. 
b. A three layer version of our model. In practice, we can constrain several of the embedding matrices to be the same

---

# Sentence Representation

```python
class MemoryLayer(nn.Module):
    def __init__(self, num_vocab: int, embedding_dim: int):
      self.A = nn.Embedding(num_vocab, embedding_dim)
      self.C = nn.Embedding(num_vocab, embedding_dim)
      
    def forward(self, query: torch.Tensor, story: torch.Tensor):
        u = self.C(query) # Batch x words x dim
        
        m_A = self.A(story) # Batch x sentences x words x dim
        m_C = self.C(story) # ^^^
```

.bottom-content[
&gt; ... the bag-of-words (BoW) representation that takes the sentence `\(x_i = \{x_{i1}, x_{i2}, ..., x_{in}\},\)` embeds each word ... &lt;span style="color: white"&gt;sums the resulting vectors: e.g `\(m_i = \sum_j Ax_{ij}\)` and `\(c_i = \sum_j Cx_{ij}\)`.&lt;/span&gt;
]

---

# Sentence Representation

```python
class MemoryLayer(nn.Module):
    def __init__(self, num_vocab: int, embedding_dim: int):
      self.A = nn.EmbeddingBag(num_vocab, embedding_dim)
      self.C = nn.EmbeddingBag(num_vocab, embedding_dim)
      
    def forward(self, query: torch.Tensor, story: torch.Tensor):
        u = self.C(query) # Batch x dim
        
        m_A = self.A(story) # Batch x sentences x dim
        m_C = self.C(story) # ^^^
```

.bottom-content[
&gt; ... the bag-of-words (BoW) representation that takes the sentence `\(x_i = \{x_{i1}, x_{i2}, ..., x_{in}\},\)` embeds each word and sums the resulting vectors: e.g `\(m_i = \sum_j Ax_{ij}\)` and `\(c_i = \sum_j Cx_{ij}\)`.
]

---

# Input Memory Representation

```python
class MemoryLayer(nn.Module):
    def __init__(self, num_vocab: int, embedding_dim: int):
      self.A = nn.EmbeddingBag(num_vocab, embedding_dim)
      self.C = nn.EmbeddingBag(num_vocab, embedding_dim)
      
    def forward(self, query: torch.Tensor, story: torch.Tensor, u: torch.Tensor = None):
        if u is None:
          u = self.C(query) # Batch x dim
        
        m_A = self.A(story) # Batch x sentences x dim
        m_C = self.C(story) # ^^^
        
        p = F.softmax((m_A @ u.unsqueeze(-1)).squeeze(-1), dim=-1)
```

.bottom-content[
&gt; In the embedding space, we compute the match between `\(u\)` and each memory `\(m_i\)` by taking the inner product followed by a softmax:
`$$p_i = \textrm{softmax}(u^T m_i)$$`
]

---

# Output Memory Representation

```python
class MemoryLayer(nn.Module):
    def __init__(self, num_vocab: int, embedding_dim: int):
      self.A = nn.EmbeddingBag(num_vocab, embedding_dim)
      self.C = nn.EmbeddingBag(num_vocab, embedding_dim)
      
    def forward(self, query: torch.Tensor, story: torch.Tensor, u: torch.Tensor = None):
        if u is None:
          u = self.C(query) # Batch x dim
        
        m_A = self.A(story) # Batch x sentences x dim
        m_C = self.C(story) # ^^^
        
        p = F.softmax((m_A @ u.unsqueeze(-1)).squeeze(-1), dim=-1)
        o_k = p.unsqueeze(1).bmm(m_C).squeeze(1)
    
        return o_k
```

.bottom-content[
&gt; The response vector from the memory `\(o\)` is then a sum over the transformed inputs `\(c_i\)`, weighted by the probability vector from the input:
`$$o = \sum_i p_i c_i$$`

]


---

# End-to-End Memory Network 

```python
class MemoryNetwork(nn.Module):
    def __init__(self, num_vocab, embedding_dim, sentence_size, max_hops):
        super().__init__()
        self.memory_layers = nn.ModuleList([MemoryLayer(
          num_vocab, 
          embedding_dim, 
          sentence_size
        ) for _ in range(max_hops)])

    def forward(self, query, story):
        u = self.memory_layers[0](query, story, u=None)
        
        for memory_layer in self.memory_layers[1:]:
            u = u + memory_layer(query, story, u)
```

.bottom-content[
&gt; The input to layers above the first is the sum of the output `\(o^k\)`
and the input `\(u^k\)` from layer `\(k\)` [...]:
`$$u^{k+1} = u^k + o^k$$`

]

???

* different ways to combine `\(o^k\)` and `\(u^k\)` are proposed later

---

# End-to-End Memory Network 

```python
class MemoryNetwork(nn.Module):
    def __init__(self, num_vocab, embedding_dim, sentence_size, max_hops):
        super().__init__()
        self.memory_layers = nn.ModuleList([MemoryLayer(
          num_vocab, 
          embedding_dim, 
          sentence_size
        ) for _ in range(max_hops)])
        self.fc_logits = nn.Linear(embedding_dim, num_vocab)

    def forward(self, query, story):
        u = self.memory_layers[0](query, story, u=None)
        
        for memory_layer in self.memory_layers[1:]:
            u = u + memory_layer(query, story, u)
       
        a_hat = self.fc_logits(u)
        return F.softmax(a_hat, dim=-1)
```

.bottom-content[
&gt;  the sum of the output vector `\(o\)` and the input embedding `\(u\)` is then passed through a final weight matrix `\(W\)` (of size `\(V Ã— d\)`) and a softmax to produce the predicted label:
`$$\hat{a} = \textrm{softmax}(W(o + u))$$`

]

---

# Weight Sharing

```python
class MemoryNetwork(nn.Module):
    def __init__(self, num_vocab, embedding_dim, sentence_size, max_hops):
        super().__init__()
        
        A = nn.EmbeddingBag(num_vocab, embedding_dim)
        C = nn.EmbeddingBag(num_vocab, embedding_dim)
        # layer-wise tying (RNN-like)
        self.memory_layers = nn.ModuleList([MemoryLayer(A, C) for _ in range(max_hops)])
        self.fc_logits = nn.Linear(embedding_dim, num_vocab)

    def forward(self, query, story):
        u = self.memory_layers[0](query, story, u=None)
        
        for memory_layer in self.memory_layers[1:]:
            u = u + memory_layer(query, story, u)
       
        a_hat = self.fc_logits(u)
        return F.softmax(a_hat, dim=-1)
```

---

# Positional Encoding

```python
class MemoryNetwork(nn.Module):
    def __init__(self, num_vocab, embedding_dim, sentence_size, max_hops):
        super().__init__()
        # Static encodings to add position information
        A = PositionalEmbeddingBag(num_vocab, embedding_dim, sentence_size)
        C = PositionalEmbeddingBag(num_vocab, embedding_dim, sentence_size)
        # layer-wise tying (RNN-like)
        self.memory_layers = nn.ModuleList([MemoryLayer(A, C) for _ in range(max_hops)])
        self.fc_logits = nn.Linear(embedding_dim, num_vocab)

    def forward(self, query, story):
        u = self.memory_layers[0](query, story, u=None)
        
        for memory_layer in self.memory_layers[1:]:
            u = u + memory_layer(query, story, u)
       
        a_hat = self.fc_logits(u)
        return F.softmax(a_hat, dim=-1)
```

---

# Positional Encoding

```python
class PositionalEncoding(nn.Module):
    def __init__(self, sentence_size, embedding_dim):
        super().__init__()
        self.register_buffer('encoding', self.get_encoding(sentence_size, embedding_dim))

    def forward(self, x, dim=-2):
        while (encoding := self.encoding).dim() &lt; x.dim():
            encoding = encoding.unsqueeze(0)
        return torch.sum(x * encoding.expand_as(x), dim=dim)

    @staticmethod
    def get_encoding(sentence_size, embedding_dim):
        encoding = torch.ones((embedding_dim, sentence_size))
        i, j = torch.cartesian_prod(
            torch.arange(1, embedding_dim + 1), 
            torch.arange(1, sentence_size + 1)
        ).t()

        encoding[(i - 1, j - 1)] = (
          (i - (embedding_dim+1)/2) * (j - (sentence_size+1)/2)
        )
        encoding = 1 + 4 * encoding / embedding_dim / sentence_size
        encoding[:, -1] = 1.0
        return encoding.t()
```

---

# Positional Encoding

```python
class PositionalEmbeddingBag(nn.Embedding):
    def __init__(self, vocab_size, embedding_dim, sentence_size, **kwargs):
        super().__init__(vocab_size, embedding_dim, **kwargs)
        self.encoding = PositionalEncoding(sentence_size, embedding_dim)

    def forward(self, x, dim: int = -2):
        embedded = super().forward(x)
        pos_aware = self.encoding(embedded, dim=dim)
        return pos_aware
```

???

* Also mention temporal encoding, time permitting


---

# Training

&lt;!-- --- --&gt;
&lt;!-- ```{r child = 'sections/results.Rmd'} --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;
&lt;!-- ```{r child = 'sections/related.Rmd'} --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;
&lt;!-- ```{r child = 'sections/conclusion.Rmd'} --&gt;
&lt;!-- ``` --&gt;

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="templates/js/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
